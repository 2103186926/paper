# 代码混淆模式特征提取解析

这段代码用于提取代码中的混淆模式特征，检测可能的恶意代码混淆技术。

## 执行过程

1. **初始化特征向量**：创建一个32维的零向量
2. **源代码检查**：如果没有源代码，直接返回零向量
3. **模式匹配**：使用正则表达式匹配各种混淆模式，统计每种模式的匹配次数
4. **模式分类**：将所有模式分为8个类别
5. **特征计算**：为每个类别计算4个特征值
6. **特征填充**：将计算的特征值填入特征向量

## 特征向量结构

返回的`features`是一个32维的一维数组，但在逻辑上可以看作是8×4的二维数组，其中：
- 8行代表8个不同的模式类别
- 4列代表每个类别的4个特征值

## 二维数组示例

假设我们有以下匹配结果：
- `base64_decode`: 3次匹配
- `eval_exec`: 2次匹配
- `dynamic_import`: 1次匹配
- 其他模式: 0次匹配

将`features`重塑为8×4的矩阵形式：

```
[
    [1.0, 1.3863, 0.2500, 1.3863],  # encoding_decoding (base64_decode有3次匹配)
    [0.0, 0.0000, 0.0000, 0.0000],  # string_manipulation (无匹配)
    [1.0, 1.0986, 0.5000, 1.0986],  # code_execution (eval_exec有2次匹配)
    [1.0, 0.6931, 1.0000, 0.6931],  # dynamic_loading (dynamic_import有1次匹配)
    [0.0, 0.0000, 0.0000, 0.0000],  # system_interaction (无匹配)
    [0.0, 0.0000, 0.0000, 0.0000],  # file_operations (无匹配)
    [0.0, 0.0000, 0.0000, 0.0000],  # network_operations (无匹配)
    [0.0, 0.0000, 0.0000, 0.0000]   # obfuscation_techniques (无匹配)
]
```

## 特征含义解释

每行的4个特征值分别表示：

1. **存在性指标**：该类别是否有匹配 (0或1)
2. **总匹配次数**：该类别所有模式的总匹配次数（对数缩放）
3. **匹配模式比例**：该类别中有多少比例的模式被匹配
4. **最大匹配次数**：该类别中单个模式的最大匹配次数（对数缩放）

## 实际存储形式

在代码中，这个二维结构实际上是线性存储的一维数组：

```
[1.0, 1.3863, 0.2500, 1.3863, 0.0, 0.0000, 0.0000, 0.0000, 1.0, 1.0986, 0.5000, 1.0986, 1.0, 0.6931, 1.0000, 0.6931, 0.0, 0.0000, 0.0000, 0.0000, 0.0, 0.0000, 0.0000, 0.0000, 0.0, 0.0000, 0.0000, 0.0000, 0.0, 0.0000, 0.0000, 0.0000]
```

这种设计使得特征向量可以直接用于机器学习算法，同时保留了类别和特征的语义结构。



# exec.json的代码混淆模式特征提取分析

我将基于exec.json文件，演示`_extract_pattern_features`方法中各个特征值的计算过程。

## 1. 模式匹配统计

首先，代码会使用正则表达式在源代码中查找各种混淆模式：

```python
pattern_counts = {}
for pattern_name, pattern_regex in CODE_PATTERNS.items():
    matches = re.findall(pattern_regex, self.source_code)
    pattern_counts[pattern_name] = len(matches)
```

假设从exec.json对应的源代码中，我们得到以下匹配结果：

```python
pattern_counts = {
    "base64_decode": 0,
    "hex_decode": 0,
    "rot13": 0,
    "char_code": 0,
    "string_concat": 1,
    "string_join": 0,
    "string_format": 3,  # f-string格式化
    "string_replace": 0,
    "eval_exec": 5,      # eval和exec调用
    "compile_code": 2,   # compile调用
    "dynamic_import": 0,
    "os_system": 0,
    "subprocess_call": 0,
    "file_open": 0,
    "file_read": 0,
    "file_write": 0,
    "network_connect": 0,
    "obfuscated_control": 0,
    "lambda_obfuscation": 0,
    "sleep_pattern": 0,
    "random_pattern": 0,
    "environment_check": 0
}
```

## 2. 类别特征计算

接下来，代码会按类别计算特征：

### 对于 "encoding_decoding" 类别:

```python
patterns = ["base64_decode", "hex_decode", "rot13", "char_code"]
```

- **category_count** = 0 + 0 + 0 + 0 = 0
- **matched_patterns** = 0 (没有匹配的模式)
- **max_count** = max([0, 0, 0, 0]) = 0
- **avg_count** = 0 / 4 = 0

### 对于 "string_manipulation" 类别:

```python
patterns = ["string_concat", "string_join", "string_format", "string_replace"]
```

- **category_count** = 1 + 0 + 3 + 0 = 4
- **matched_patterns** = 2 (string_concat和string_format有匹配)
- **max_count** = max([1, 0, 3, 0]) = 3
- **avg_count** = 4 / 4 = 1

### 对于 "code_execution" 类别:

```python
patterns = ["eval_exec", "compile_code"]
```

- **category_count** = 5 + 2 = 7
- **matched_patterns** = 2 (两个模式都匹配)
- **max_count** = max([5, 2]) = 5
- **avg_count** = 7 / 2 = 3.5

### 对于 "dynamic_loading" 类别:

```python
patterns = ["dynamic_import"]
```

- **category_count** = 0
- **matched_patterns** = 0
- **max_count** = 0
- **avg_count** = 0

### 对于其他类别:

由于没有匹配，所有值都是0

## 3. 特征向量填充

对于每个类别，代码会计算4个特征值并填入特征向量：

```python
features[i * 4:i * 4 + 4] = [
    1.0 if category_count > 0 else 0.0,  # 是否存在该类别的模式
    np.log1p(category_count),            # 总匹配次数（对数缩放）
    matched_patterns / len(patterns) if patterns else 0,  # 匹配模式比例
    np.log1p(max_count)                  # 最大匹配次数（对数缩放）
]
```

## 4. 最终特征向量

将`features`重塑为8×4的矩阵形式：

```
[
    [0.0, 0.0000, 0.0000, 0.0000],  # encoding_decoding
    [1.0, 1.6094, 0.5000, 1.3863],  # string_manipulation
    [1.0, 2.0794, 1.0000, 1.7918],  # code_execution
    [0.0, 0.0000, 0.0000, 0.0000],  # dynamic_loading
    [0.0, 0.0000, 0.0000, 0.0000],  # system_interaction
    [0.0, 0.0000, 0.0000, 0.0000],  # file_operations
    [0.0, 0.0000, 0.0000, 0.0000],  # network_operations
    [0.0, 0.0000, 0.0000, 0.0000]   # obfuscation_techniques
]
```

## 特征向量解释

这个32维向量（8类别×4特征）清晰地反映了exec.json文件的特征：

1. **主要特征**:
   
   - 代码执行类别（code_execution）有最高的匹配次数和完整的模式覆盖
   - 字符串操作类别（string_manipulation）也有明显的匹配
   
2. **安全相关解读**:
   
   - 高code_execution值表明代码中有多处eval、exec和compile调用
   - 这些调用与字符串操作结合可能表示潜在的代码注入风险
   
3. **实际存储形式**:
   在代码中，这个二维结构实际上是线性存储的一维数组：
   ```
   [0.0, 0.0, 0.0, 0.0, 1.0, 1.6094, 0.5, 1.3863, 1.0, 2.0794, 1.0, 1.7918, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
   ```

这种特征向量设计能够有效捕捉代码中的混淆模式，特别是与代码执行相关的潜在危险模式。

# 特征向量填充值计算示例

我将以上次回答中的两个类别为例，详细解释特征向量填充值的计算过程。

## 示例1: "string_manipulation" 类别

对于字符串操作类别，我们有以下统计数据：
- **patterns** = ["string_concat", "string_join", "string_format", "string_replace"]
- **pattern_counts** = {string_concat: 1, string_join: 0, string_format: 3, string_replace: 0}
- **category_count** = 1 + 0 + 3 + 0 = 4
- **matched_patterns** = 2 (string_concat和string_format有匹配)
- **max_count** = max([1, 0, 3, 0]) = 3
- **len(patterns)** = 4

### 特征值计算:

1. **存在性指标**:
   
   ```python
   1.0 if category_count > 0 else 0.0
   ```
   category_count = 4 > 0，所以值为 1.0
   
2. **总匹配次数（对数缩放）**:
   
   ```python
   np.log1p(category_count)
   ```
   np.log1p(4) = log(1 + 4) = **ln(5)**‌ ≈ 1.6094
   
3. **匹配模式比例**:
   
   ```python
   matched_patterns / len(patterns)
   ```
   2 / 4 = 0.5 (50%的模式被匹配)
   
4. **最大匹配次数（对数缩放）**:
   
   ```python
   np.log1p(max_count)
   ```
   np.log1p(3) = log(1 + 3)  = ln(4) ≈ 1.3863

### 最终填充值:
```
[1.0, 1.6094, 0.5000, 1.3863]
```

## 示例2: "code_execution" 类别

对于代码执行类别，我们有以下统计数据：
- **patterns** = ["eval_exec", "compile_code"]
- **pattern_counts** = {eval_exec: 5, compile_code: 2}
- **category_count** = 5 + 2 = 7
- **matched_patterns** = 2 (两个模式都匹配)
- **max_count** = max([5, 2]) = 5
- **len(patterns)** = 2

### 特征值计算:

1. **存在性指标**:
   ```python
   1.0 if category_count > 0 else 0.0
   ```
   category_count = 7 > 0，所以值为 1.0

2. **总匹配次数（对数缩放）**:
   ```python
   np.log1p(category_count)
   ```
   np.log1p(7) = log(1 + 7) ≈ 2.0794

3. **匹配模式比例**:
   ```python
   matched_patterns / len(patterns)
   ```
   2 / 2 = 1.0 (100%的模式被匹配)

4. **最大匹配次数（对数缩放）**:
   ```python
   np.log1p(max_count)
   ```
   np.log1p(5) = log(1 + 5) ≈ 1.7918

### 最终填充值:
```
[1.0, 2.0794, 1.0000, 1.7918]
```

## 特征向量在数组中的位置

假设"string_manipulation"是第2个类别(i=1)，"code_execution"是第3个类别(i=2)，那么在32维特征向量中的位置为：

- **string_manipulation**: features[4:8] = [1.0, 1.6094, 0.5000, 1.3863]
- **code_execution**: features[8:12] = [1.0, 2.0794, 1.0000, 1.7918]

这种设计使得每个类别的特征占据连续的4个位置，便于后续的机器学习模型理解和处理。



# 混淆模式特征提取过程详细分析

以`code_execute.py`为例，我将详细分析混淆模式特征的提取过程。这个文件包含了多个使用`eval`、`exec`和`compile`函数的示例，这些是常见的代码执行和可能用于混淆的函数。

## 1. 基础准备

首先，代码加载`code_execute.py`文件内容，并进行初始预处理：

```python
code_to_analyze = self.source_code or self.high_risk_code
# 在这个例子中，code_to_analyze 将包含 code_execute.py 的内容
```

## 2. 计算代码熵值

熵值是信息理论中衡量随机性或不确定性的指标。代码混淆通常会增加代码的熵值：

```python
code_entropy = self._calculate_entropy(code_to_analyze)
```

计算过程：
1. 统计每个字符出现的频率
2. 计算概率分布
3. 应用熵公式：H = -Σ(p_i * log2(p_i))

对于`code_execute.py`，计算如下：
- 文件包含各种字符（字母、数字、空格、符号等）
- 假设得到的熵值约为 4.2（比正常代码略高，因为包含多个函数定义和特殊操作）
- 但低于5.0（严重混淆代码的阈值）

## 3. 匹配混淆模式

代码用预定义的正则表达式匹配常见混淆模式：

```python
pattern_counts = {}
for pattern_name, pattern_regex in CODE_PATTERNS.items():
    matches = re.findall(pattern_regex, code_to_analyze)
    pattern_counts[pattern_name] = len(matches)
```

在`code_execute.py`中，匹配结果大致为：

| 模式名称          | 正则表达式                                 | 匹配数量 | 匹配内容示例                            |
| ----------------- | ------------------------------------------ | -------- | --------------------------------------- |
| eval_exec         | `r"eval\(|exec\("`                         | 9        | `eval("1 + 2")`, `exec(code)`           |
| compile_code      | `r"compile\("`                             | 2        | `compile(code_str, "<string>", "eval")` |
| string_format     | `r"format\(|%[sd]|f['\"]"`                 | 11       | `f"eval result: {result}"`              |
| dynamic_import    | `r"__import__\(|importlib\.import_module"` | 0        | 无                                      |
| other patterns... | ...                                        | 大多为0  | 无                                      |

## 4. 检查高危模式

如果提供了高危测试文件，还会检查更具体的高危模式：

```python
high_risk_counts = {}
if self.high_risk_code:
    for category, patterns in HIGH_RISK_PATTERNS.items():
        category_matches = []
        for pattern in patterns:
            matches = re.findall(pattern, self.high_risk_code)
            category_matches.extend(matches)
        high_risk_counts[category] = len(category_matches)
```

对于`code_execute.py`，检测到的高危类别主要是：

| 高危类别            | 匹配数量 | 匹配内容示例                  |
| ------------------- | -------- | ----------------------------- |
| code_execution      | 9        | `eval("1 + 2")`, `exec(code)` |
| other categories... | 大多为0  | 无                            |

## 5. 特征分组和计算

接下来，代码将这些模式按类别分组，并为每个类别计算特征：

```python
pattern_categories = {
    "encoding_decoding": ["base64_decode", "hex_decode", "rot13", "char_code"],
    "string_manipulation": ["string_concat", "string_join", "string_format", "string_replace"],
    "code_execution": ["eval_exec", "compile_code"],
    ...
}
```

对于`code_execute.py`，各类别的统计结果大致为：

1. **encoding_decoding**: 匹配数量≈0（无base64或hex编码）
2. **string_manipulation**: 匹配数量≈11（主要是string_format匹配）
3. **code_execution**: 匹配数量≈11（eval_exec匹配9次，compile_code匹配2次）
4. **dynamic_loading**: 匹配数量≈0
5. **system_interaction**: 匹配数量≈0
6. **file_operations**: 匹配数量≈0
7. **network_operations**: 匹配数量≈0
8. **obfuscation_techniques**: 匹配数量≈0

## 6. 计算类别特征

对每个类别，计算四个特征值，构成32维特征向量：

```python
# 为每个类别计算特征
for i, (category, patterns) in enumerate(pattern_categories.items()):
    # 计算该类别的总匹配次数
    category_count = sum(pattern_counts.get(pattern, 0) for pattern in patterns)
    
    # 高危因子和熵因子增强处理...
    
    # 填充特征向量
    features[i * 4:i * 4 + 4] = [
        1.0 if category_count > 0 else 0.0,  # 是否存在该类别的模式
        np.log1p(category_count),            # 总匹配次数（对数缩放）
        matched_patterns / len(patterns) if patterns else 0,  # 匹配模式比例
        np.log1p(max_count)                  # 最大匹配次数（对数缩放）
    ]
```

以code_execution类别为例，详细计算过程：

1. **总匹配次数** = eval_exec匹配(9) + compile_code匹配(2) = 11
2. **存在指标** = 1.0（因为有匹配）
3. **对数匹配次数** = np.log1p(11) ≈ 2.4849
4. **匹配模式比例** = 2/2 = 1.0（两个模式都匹配到了）
5. **最大匹配对数** = np.log1p(9) ≈ 2.3026（eval_exec的9次匹配）

因此，code_execution类别的四个特征是：[1.0, 2.4849, 1.0, 2.3026]

## 7. 应用熵增强和高危增强

对特定类别，根据代码熵值或高危检测结果进行特征增强：

```python
# 高危增强
if category in high_risk_counts:
    high_risk_factor = min(1.0, high_risk_counts[category] / 10.0)
    category_count = max(category_count, int(high_risk_factor * 10))

# 熵增强
if category in ["obfuscation_techniques", "encoding_decoding"] and code_entropy > 4.5:
    entropy_factor = min(1.0, (code_entropy - 4.5) / 1.5)
    category_count = max(category_count, int(entropy_factor * 10))
```

对于`code_execute.py`：
- **高危增强**：code_execution类别会得到增强，因为高危检测到了9个匹配
- **熵增强**：假设熵值为4.2，低于4.5阈值，所以不会触发熵增强

## 8. 完整的32维特征向量

最终的32维特征向量（简化展示）：

| 类别索引 | 类别名称               | 特征1 | 特征2 | 特征3 | 特征4 |
| -------- | ---------------------- | ----- | ----- | ----- | ----- |
| 0        | encoding_decoding      | 0.0   | 0.0   | 0.0   | 0.0   |
| 1        | string_manipulation    | 1.0   | ~2.5  | 0.25  | ~2.5  |
| 2        | code_execution         | 1.0   | ~2.5  | 1.0   | ~2.3  |
| 3        | dynamic_loading        | 0.0   | 0.0   | 0.0   | 0.0   |
| 4        | system_interaction     | 0.0   | 0.0   | 0.0   | 0.0   |
| 5        | file_operations        | 0.0   | 0.0   | 0.0   | 0.0   |
| 6        | network_operations     | 0.0   | 0.0   | 0.0   | 0.0   |
| 7        | obfuscation_techniques | 0.0   | 0.0   | 0.0   | 0.0   |

## 9. 混淆检测结论

基于特征计算，`detect_obfuscation`方法会判断代码是否混淆：

```python
obfuscation_indicators = {
    "高熵值": bool(entropy > 5.0),
    "长行": bool(avg_line_length > 100),
    "base64编码": bool(re.search(r'base64\.(b64decode|decodestring)', code_to_analyze)),
    "eval/exec": bool(re.search(r'eval\(|exec\(', code_to_analyze)),
    "变量名混淆": bool(re.search(r'\b[a-zA-Z]{1,2}\d*\b', code_to_analyze) and 
                   not re.search(r'def\s+[a-zA-Z]{3,}', code_to_analyze)),
    "字符串拼接": bool(code_to_analyze.count('+') > 20 and code_to_analyze.count("'") > 20)
}
```

对于`code_execute.py`的检测结果：
- **高熵值**: False (熵值约4.2，小于5.0)
- **长行**: False (平均行长约30-40，远小于100)
- **base64编码**: False (无base64编码)
- **eval/exec**: True (含有多个eval和exec调用)
- **变量名混淆**: False (使用了正常的变量名)
- **字符串拼接**: False (字符串拼接不多)

最终判断：
- 仅有1个指标为True，小于阈值2
- 结论：`code_execute.py`不是混淆代码，虽然使用了eval/exec，但用法正常且清晰

这个例子很好地展示了本算法的工作原理：它不仅考虑危险函数的存在，还关注代码的整体结构和特征，从而准确区分合法使用eval/exec的清晰代码和真正的混淆代码。



我来详细解释代码熵值的计算过程。以`code_execute.py`为例，我们一步步分析：

# 熵值计算原理
熵值计算基于信息论，用于衡量信息的不确定性。代码的熵值越高，表示代码的随机性或复杂性越高。

## 计算步骤

### 步骤1：统计字符频率
首先统计文件中每个字符出现的次数：

```python
# 示例代码片段（以code_execute.py的前几行为例）
text = """# exec.py
# 用于AST分析的测试用例，展示eval、exec、compile的用法
# 所有代码均为安全、无害的示例"""

char_freq = {}
for char in text:
    if char in char_freq:
        char_freq[char] += 1
    else:
        char_freq[char] = 1
```

对于`code_execute.py`，部分字符频率统计结果可能如下：
```python
char_freq = {
    'e': 15,  # e出现15次
    ' ': 12,  # 空格出现12次
    '#': 3,   # #号出现3次
    'A': 2,   # A出现2次
    'S': 2,   # S出现2次
    'T': 2,   # T出现2次
    ...
}
```

### 步骤2：计算概率分布
计算每个字符出现的概率：

```python
length = len(text)  # 总字符数
probabilities = {}
for char, freq in char_freq.items():
    probabilities[char] = freq / length
```

例如：
```python
probabilities = {
    'e': 15/112,  # ≈ 0.134
    ' ': 12/112,  # ≈ 0.107
    '#': 3/112,   # ≈ 0.027
    ...
}
```

### 步骤3：计算熵值
使用香农熵公式计算：H = -Σ(p_i * log2(p_i))

```python
entropy = 0.0
for probability in probabilities.values():
    entropy -= probability * np.log2(probability)
```

具体计算：
```python
# 部分计算示例
entropy = -(
    0.134 * log2(0.134) +    # e的贡献
    0.107 * log2(0.107) +    # 空格的贡献
    0.027 * log2(0.027) +    # #号的贡献
    ...
)
```

## 3. 完整示例代码

让我们用实际代码计算`code_execute.py`的熵值：

```python
def calculate_entropy_example(file_path):
    # 读取文件内容
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    # 1. 统计字符频率
    char_freq = {}
    for char in text:
        if char in char_freq:
            char_freq[char] += 1
        else:
            char_freq[char] = 1
    
    # 2. 计算总长度
    length = len(text)
    
    # 3. 计算熵值
    entropy = 0.0
    for freq in char_freq.values():
        probability = freq / length
        entropy -= probability * np.log2(probability)
    
    # 打印详细信息
    print(f"文件长度：{length}字符")
    print("\n最常见的10个字符及其概率：")
    sorted_chars = sorted(char_freq.items(), key=lambda x: x[1], reverse=True)[:10]
    for char, freq in sorted_chars:
        if char.isspace():
            char_display = f"空格"
        elif char == '\n':
            char_display = f"换行"
        else:
            char_display = char
        probability = freq / length
        print(f"字符'{char_display}': 频率={freq}, 概率={probability:.4f}, "
              f"熵贡献={-probability * np.log2(probability):.4f}")
    
    print(f"\n总熵值：{entropy:.4f} 比特/字符")
    return entropy
```

## 4. 解释结果

对于`code_execute.py`，熵值计算结果大约在4.2-4.5之间，这是因为：

1. 代码包含大量的英文字母、空格和常见符号，分布相对均匀
2. 有规律的缩进和换行使得某些字符（空格、换行）出现频率较高
3. 代码结构清晰，没有混淆处理，所以熵值处于正常范围

相比之下：
- 普通英文文本的熵值约为4.0-4.3
- 随机字符串的熵值可能高达5.5-6.0
- 混淆后的代码熵值通常在5.0以上

在`detect_obfuscation`函数中，使用熵值>5.0作为可能存在混淆的阈值：
```python
obfuscation_indicators = {
    "高熵值": bool(entropy > 5.0),
    # ...
}
```

`code_execute.py`的熵值低于5.0，这表明它是一个正常的、未混淆的代码文件，这与我们的预期相符，因为它是一个用于测试的清晰示例代码。